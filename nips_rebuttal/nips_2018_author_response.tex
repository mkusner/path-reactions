\documentclass{article}

\usepackage{nips_2018_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{wrapfig}
\usepackage{bm}


\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}


% Our model
\newcommand{\ourModel}{\textsc{Electro}}
\newcommand{\ourModelIR}{\textsc{Electro-Lite}}
\newcommand{\ourModelR}{\textsc{Electro}}

\begin{document}

%% ============================================
%% ========= Intro Section  ===================
%% ============================================


\begin{wrapfigure}{r}{0.6\textwidth}
\centering
  \includegraphics[width=10cm]{graph_nn.pdf}
 \caption{Thumbnail of new image we propose  putting in Section 3 of the paper. We hope that this (with the associated text) makes the overall workflow for forming node and graph embeddings via a message passing framework (following the approaches in [9-10]) clearer.}
 \label{fig:new-diagram}
\end{wrapfigure}

Thank you for taking the time to review our paper. 
\todo[]{Some changes to fig: a. missing vectors for some atoms, b. make super explicit that A shared by changing sub index to single and double bond, c. use table to refer to explicitly refer to Table 1 in SM for more details, d. make it clear that boxes (2) and (3) happen multiple times; e. Change the sum to be over mathcal A'}

We are grateful for your suggestions about improving the description of the model. We therefore have made a few changes to section 3 to improve clarity.
In particular, we add a step-by-step description of the workflow and a new diagram, Figure \ref{fig:new-diagram}, to provide background on GGNNs and how these can be used to provide graph and node embeddings. 
In addition we have brought out of the supplementary material some details on more specific architecture choices and to have space to do so, we are grateful to R2's suggestion to move the more technical details of section 4 into the appendix.

To the more specific individual comments we respond to each reviewer below.

%% ============================================
%% ========= Reviewer 1  ======================
%% ============================================

\paragraph{Reviewer 1}

\emph{(2) Why the present method is expected to perform better than WLDN in product prediction?}
We believe that as Electro is much closer to the underlying physical reality of reaction mechanisms being composed of sequential electron redistribution steps, it provides the needed compositional and relational inductive bias, leading to better performance.
 We add however, that aside from the improved product prediction performance, we believe that (1) the interpretability provided by Electro by modelling the mechanism steps and (2) the avoidance of having to set up a somewhat more fragmented machine learning pipeline of predict-then-rank steps are also advantageous features of our model.

\emph{(3) How the present model compares to WLDN and seqtoseq in terms of runtime.}
For predicting products, using a Beam search width of 10, Electro takes 0.337 secs. This compares to 0.034secs for WLDN [5] and 0.025\footnote{As no code is available for this paper we quote their stated time.} for the Seq2Seq model [14, \S6.2]. 
\todo[]{Check wldn timings as they give 50ms in paper. Maybe they include the initial rdkit time.}
 However, the majority of the time for Electro (0.193 secs) is used within RDKit to create intermediate molecules or derive features from them, an operation that is not currently parallelized across the different beams; only 0.044 secs is used given the features to compute the action probabilities.
  At test time we take advantage of the embarrassingly parallel nature of the task to parallelize across test inputs. 
   If we are interested in only computing the log likelihood of a reaction (and so have access to all the intermediate steps), this takes 0.007 secs on a GPU for Electro. \todo[]{Can also add the fact we have less parameters.}

%The timings of the models are given in Table \ref{table:timings}. We shall include these in the paper.
% For Electro we use a beam width of ten. 
% Although the overall time is greater than the others, only 0.044 secs is used to compute the probabilities and the majority of the time (0.193 secs) is used using RDKIT to create intermediate molecules or derive features from them, an operation that is not currently parallelized across the different beams. 
% Instead as the whole predict operation for Electro only takes 0.428 secs on a CPU we take advantage of the embarrassingly parallel nature of the task across different reaction predictions when running Electro on the test set.
% If we are interested in only computing the log likelihood of a reaction (and so have access to all the intermediate steps), this takes 0.007 secs on a GPU for Electro.
% 
%\begin{table}[h]
%  \caption{Time taken in seconds to predict product from the initial reactant and reagent molecules.
%  These have been run on a NVIDIA 1080 GPU, with the mean time taken over the first 500 items of the test set.
%  * As no code is available for [14] we instead report the timings given in Section 6.2 of their paper.
%  }
%  \label{table:timings}
%  \centering
%  \begin{tabular}{llll}
%    \toprule
%    & Electro (Ours) & WLDN [5] & Seq2Seq [14]  \\
%    \midrule
%    Timing (secs) & 0.337   & 0.034 &  0.025*     \\
%    \bottomrule
%  \end{tabular}
%\end{table}



%% ============================================
%% ========= Reviewer 2  ======================
%% ============================================

\paragraph{Reviewer 2}
\emph{Point i)}
The motivation of only including reagent context at the start is that this is often the most challenging decision, and that action steps after this have access to the previous atom as context, making it an easier task.
%Qualitatively, when running Electro-lite on the separate validation set we would see that the model often had the most errors on the first step, and that after picking this first step the next stages would often be correctly predicted.
We have since tested a version of \ourModel where reagent information is fed in as context at each step. 
On the mechanism prediction task (Table 1) this gets a slightly improved top-1 accuracy of 78.4\% but a similar top-5 accuracy of 94.6\%.

%One of the nice features of the approach is that we can easily break down the loss (or equivalently the negative log likelihood) of each prediction our model makes. 
%Empirically we found that Elctro-lite on the first 500 items of the validation set obtained a median loss of 0.209 on predicting where to start, whereas the first add step (when applicable) would contribute a median loss of 0.002, highlighting the sometimes much poorer performance of this module.
%When testing Electro, its initial select module would get a median loss of 0.023, showing that the addition of reagents, greatly improves the prediction of this task.

\emph{Point iii)} 
Sorry for the confusion here. In eq 4, $\bm{H}_m$ should be $\bm{H}_{\mathcal{M}_t}$. We apologize for this typo.
In eq 2 $\mathcal{A}’$ is the set of all atoms in the graphs under consideration, so for instance when forming the reagent context vector this will be only the reagents.
The dependency on $a_{t-1}$ comes about by the changes in the molecular graph, caused by action $a_{t-1}$. 
To clarify this, we will add equations for each of our predictors: $f_\textrm{start}$, $f_\textrm{add}$, $f_\textrm{remove}$ (the structure of which are currently detailed in the SM).

\emph{Point v)} 
We thought a lot about how to compare with [6].
 Ultimately we decided we were unable to compare with it (and also [7]) for the following reasons:
  1.These methods, as they currently stand, cannot run on our data as they currently can not run on all the elements present in USPTO [6, p.2528], [7, p.2214] and, furthermore, require access to (the unknown) reaction conditions. 
 2. We cannot run our method on their data as the link to their data is missing atom information, and they did not respond to our emails asking for their code or data. 

\emph{Point vi)} We have run the WLDN[5]'s code for the case where it is also trained on our filtered training dataset.
 We found it achieved top-1, top-3 and top-5 accuracies of 83.1\%, 91.5\% and 92.7\% respectively. 
 We hope that this rules out \ourModel having better performance due to a more specialized training task.
  
\emph{Point vii)}
 We politely disagree here.
  We think it is completely surprising that the model is able to pick up subtle chemical trends (e.g., only single atomic changes) without being explicitly trained on this task.
   We think an experiment like this may give additional confidence to a chemist that our model has learned reaction prediction, as this experiment resembles exam questions for organic chemistry students. \todo[]{could say we will motivate this reasoning better}
 
\emph{Point viii)}
We believe this work advances two novel directions for future research:

 1. We observe that reactions are essentially stepwise graph-editing procedures. 
 It’s fruitful to model it this way precisely because it sets the stage for models that work on more general types of chemical reactions.
It also potentially can be applied on graph editing based applications outside the domain of chemistry.

  2. More broadly, there is a significant amount of structure and powerful abstractions developed in chemistry that we as machine learning researchers can take advantage of. 
Modeling using these existing abstractions aids interoperability and the interrogation of trained models.
In \ourModel, we have a successful model that operates using the effective abstractions of reaction mechanism.
We show how using mechanisms we have a model that does the same sorts of things that organic chemistry students learning reactions do, and is a model much easier to work with, criticize, and improve. 
We hope that this in turn encourages a direction of research in which existing useful abstractions and tools in application areas are harnessed when developing new ML algorithms.

 
\emph{Point ii and iv)} Thanks for these comments, we hope these have been addressed by our discussion at the top.



%% ============================================
%% ========= Reviewer 3  ======================
%% ============================================
\paragraph{Reviewer 3}
\emph{"On one hand, theis paper includes several modelling ideas that might have the potential to indeed advance the field. On the other hand, from a chemical perspective the model is still rather limited, since only topological molecule descriptors are used, neglecting the influence of conformational changes."}
%\emph{Point under EVALUATION, on significance)}
Thank you for recognizing and commending our modelling ideas. We agree conformational change information is certainly useful. So far, work at machine learning venues has yet to make use of these conformational change features for reaction prediction, but we aim to consider this in future work on more general reaction classes. That said, in this current work, our purely topological model is harnessing an extremely powerful arrow-pushing abstraction for chemists. It allows chemists to make very quick, but tremendously accurate predictions without doing any quantum simulations, and to understand relations between reaction classes, which is not possible using quantum mechanics alone. 


\end{document}
