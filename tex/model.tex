
\section{Model}
In this section we describe the structure of our model, that can, given an initial set of reactant molecules $\moleculeSet_0$, and a set of reagent molecules $\moleculeSet_r$, produce a path of electron movements: $\electronPath_{0:T} = (a_0, a_1, \ldots, a_{T})$. This path of electron movements via a known deterministic function, yields a sequence of sets of intermediate and final products. We denote the sets of molecules create by an action sequence $\electronPath_{0:t} = (a_0, a_1 \ldots, a_{t})$ as $\moleculeSet_{\electronPath_{0:t}}$. 

We propose to learn a parameterized distribution over electron movements: $p_\theta( \electronPath_{0:T} \mid \moleculeSet_0, \moleculeSet_r)$. We begin by describing the generative process (i.e., the forward pass) of $p_\theta$ and then describe how to train it.

% Because we do not know the true path $\Pc$, we can only receive a learning signal from the final predicted product molecules $\hat{\Mc}_T$ (compared to the true final molecules $\Mc_T$. This final predicted product is a deterministic (known) function $f$ of the initial molecules $\Mc_0$ and predicted actions $\hat{\Pc}$ (from $g$), as such $f(\Mc_0, \hat{\Pc}) = \hat{\Mc}_2, \ldots, \hat{\Mc}_T$.

% Ideally, we would like to learn the parameters of $g$, called $\theta$, to minimize the difference between the predicted final molecules $\hat{\Mc}_T$ and $\Mc_T$ (i.e., via some loss function $\ell$). However, we cannot resort to gradient-based techniques to learn $g$ because the inputs and outputs of $g$ are discrete objects, and the function $f$ producing $\hat{\Mc}_T$ is also discrete so the gradient $\frac{\partial \ell(\hat{\Mc}_T,\Mc_T)}{\partial \theta}$ does not exist. To solve this, we propose two types of models for this problem.





\subsection{Generative process}
The distribution over electron paths can be factorized as follows:
%\begin{align}
%p_\theta(\electronPath \mid \moleculeSet_0) = p_\theta(s_{0}' \mid \moleculeSet_0) p_\theta(a_{0} \mid \moleculeSet_0) \prod_{t=1}^{T-1} \left( p_\theta(a_{t} \mid a_{t-1}, \Mc_{t-1} ) p_\theta(s_{0}' \mid \moleculeSet_0) \right) p_\theta(a_{T} \mid a_{T}, \Mc_{T-1} ) p_\theta(s_{T} \mid \moleculeSet_T) \nonumber
%\end{align}

\begin{align*}
p(\electronPath_{0:T} \mid \initialAndReactants) = &
 \quad 
 p(s_0' \mid \initialAndReactants)
 p(a_0 \mid \initialAndReactants)
 \prod_{t=1}^{T} \Big[ 
 	p(s_t' \mid \initialAndReactants, \electronPath_{0:t-1})
 	p(a_t \mid \initialAndReactants, \electronPath_{0:t-1} ) 
 \Big] \\
 & \times p(s_{T+1} \mid  \initialAndReactants, \electronPath_{0:T} )
\end{align*}

\improvement[]{this part a bit messy}
Where $p(s_{t})$ is the probability of stopping the path before picking the $t^{\text{th}}$ action, $p(s_{t}')$ of continuing.
In practice we assume that the probability of an action depends only on (i) the intermediate molecule formed by the action path up to that point, (ii) the previous action taken (indicating where the free pair of electrons are) and (iii) the point of time through the path, indicating whether we are on an add or remove bond step. 
We further make the simplifying assumption that the stop probability and the actions after $a_0$ do not depend on the reagents. This leads to our parameterized model:

\begin{align*}
p_\theta(\electronPath_{0:T} \mid \moleculeSet_0, \moleculeSet_r) = &
 \quad \continueProb{0}{0}
       p(a_0 \mid \moleculeSet_0, \moleculeSet_r)
       p(a_1 \mid \moleculeSet_0, a_0) \\
       & \times \prod_{t=2}^{T} \Big[
              \continueProb{t}{\electronPath_{0:t-1}}
              \actionProb{t}
       \Big] \\
       & \times p(s_{T+1} \mid \moleculeSet_{\electronPath_{0:T}})
\end{align*}

Note that you cannot stop after one action, as you have to pick up a complete electron pair.
As described in the previous section, if the three reaction assumptions above hold, then there are two types of electron movements that alternate: 1. movement that \emph{removes an existing bond}, and 2. movement that \emph{adds a new bond}. We can generalize assumption 3 by defining that atoms with free electrons have a self-bond. Thus, all reactions start by first selecting an atom, removing a bond (between two different atoms, or a self-bond), and then alternately adding and removing bonds. 

\unsure[]{This part is not quite right. We currently have a f second, although Im training models that no longer have this}
We parameterize remove-bond, add-bond, and start-atom steps in similar ways. Specifically, the functional form of a remove-bond step is described by the following equations:
\begin{align}
\Hb_{\Ac} \;&= f_{\Ac}(\moleculeSet_{\electronPath_{0:t-1}}) \label{eq:rem_atom} \\
%\Hb_{\Bc} \;&= f_{\Bc}(\Mc_t, \Hb_{\Ac}) \label{eq:rem_bond} \\
p(a_t \mid \moleculeSet_{\electronPath_{0:t-1}}, a_{t-1}) \;&= \mbox{softmax}(f_{\textrm{remove}}(\Hb_{\Ac}, a_{t-1})). \label{eq:rem_prob}\end{align}
Recall that $\Mc_t$ is a deterministic function of $\Mc_0$ and $a_{0:t-1}$.

Function $f_{\Ac}$ in eq.~\eqref{eq:rem_atom} takes a molecular graph and for each atom outputs a set of $d$-dimensional atom features, so that $\Hb_{\Ac} \subseteq \mathcal{R}^{|\Ac| \times d}$. 
In general $f_{\Ac}$ could be any deep graph model that uses the graph structure of $\Mc_t$ to get graph-isomorphic node features, usually via message-passing techniques \citep{gilmer2017neural}. We choose to use Gated Graph Neural Network message functions \citep{li2016gated}.
In eq.~\eqref{eq:rem_prob}, $f_{\textrm{remove}}$ followed by a $\mbox{softmax}$ function maps atom features $\Hb_{\Ac}$ and the previous action, $a_{t-1}$, to probabilities. The previous action, $a_{t-1}$, is also used to mask the resulting probabilities as follows:\improvement[]{Maybe confusing having this and also the eqn 2. above. Maybe should merge.}

\begin{align}
p(a \mid \moleculeSet_{\electronPath_{0:t-1}}, a_{t-1})) \propto 
\begin{cases}
\mbox{softmax}(f_{\textrm{remove}}(\Hb_{\Ac})) & \mbox{if bond } (a_{t-1},a) \mbox{ exists in } \moleculeSet_{\electronPath_{0:t-1}} \\
0 & \mbox{otherwise}. \nonumber
\end{cases}
\end{align}
When removing the initial bond we also allow the removal of self bonds.

\improvement[]{add time indexing to the node features}
For an add-bond step, we again use eq.~\eqref{eq:rem_atom} to obtain atom features $\Hb_{\Ac}$ and use a different function $f_{\textrm{add}}$ to produce add bond probabilities as follows: $p(a_t \mid a_{0:t-1}, \Mc_0) = \mbox{softmax}(f_{\textrm{add}}(\Hb_{\Ac}), a_{t-1})$
% \begin{align}
% \Hb_{\Ac} \;&= f_{\Ac}(\Mc_t) \label{eq:add_atom}  \\
% p(a_t \mid a_{0:t-1}, \Mc_0) \;&= \mbox{softmax}(f_{\textrm{add}}(\Hb_{\Ac}, a_{t-1})). \label{eq:add_prob}
% \end{align}
%The primary difference here is instead of a distribution over existing bonds, the distribution in eq.~\eqref{eq:add_prob} is over all possible atoms $\Ac$ (and a `null' action), as a bond may possibly form between any two atoms. 
Again we use the previous action $a_{t-1}$ to mask probabilities (here we only mask out the probability of adding a self-bond to the current atom).

We calculate stop probabilities as 
$p(s_t \mid \moleculeSet_{\electronPath_{0:t-1}} ) = \mbox{sigmoid}(\fStop(\Hb_{\Ac}))$.
Here $\fStop$ is similar to the readout functions in \citet[eq. 4]{gilmer2017neural}. In particular,
$\fStop(\Hb_{\Ac}) = k \left( \sum_{v \in \Ac} \left[ \mbox{sigmoid}(i(\Hb_{\Ac,v})) \cdot j(\Hb_{\Ac,v}) \right] \right)$.
In which $i$, $j$ and $k$ are functions parameterized by neural networks, with $k$ projecting the graph down to one dimension \improvement[]{well actually just linear layers... Two things here 1. slightly different to Gilmer. 2. When doing reagents forget to mask out empty nodes when doing the sum, so they will have whatever biases the network learns...}.


Finally, in order to select the starting atom of a path we propose to model the initial probability as follows: $p(a_0 \mid \moleculeSet_0, \moleculeSet_r) = \mbox{softmax}(f_{\textrm{start}}(\Hb_{\Ac}, \fReagEmbed(\moleculeSet_r)))$. There is no masking used for this step. Here $\fReagEmbed$ returns an embedding of the reagents. This is created by (i) computing node features for the reagents, using $f_{\Ac}$, (ii) calculating a graph embedding, by using a similar function to $\fStop$ but one which calculates multidimensional features. And then finally (iii) these features for each reagent are summed  to produce a final embedding so that we are invariant to the ordering of reagents.

\paragraph{Training}
We can learn the parameters of $\fModules$, by minimizing the negative log-likelihood of the true path:
\begin{align*}
  \min_{\fModules}
    & - \log \continueProb{0}{0} - \log p(a_0 \mid \moleculeSet_0, \moleculeSet_r)  - \log p(a_1 \mid \moleculeSet_0, a^*_0) \\
	& - \sum_{t=2}^{T} \log \Big[ \continueProb{t}{\electronPath_{0:t-1}^*} \actionProb[*]{t} \Big] \\
    & - \log p(s_{T+1} \mid \moleculeSet_{\electronPath_{0:T}^*})
\end{align*}


where $a_t^*$ and $\electronPath_{0:t-1}^*$ are the true electron actions or path of actions respectively so that we are using teacher forcing \citep{williams1989learning} during training. This allows us to train on all stages of the reaction at once.

\paragraph{Sampling}
Once trained we can sample chemically-valid paths from our model using beam search.  This is procedure is described in Algorithm~\ref{algo:valid_path}. 


\input{tex/algorithm.tex}
